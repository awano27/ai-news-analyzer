"""
ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ: ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒ»åˆ†æã‚’å®Ÿè¡Œï¼ˆå®Œå…¨ç„¡æ–™ç‰ˆï¼‰
"""

import os
import sys
import json
import logging
from datetime import datetime
from dotenv import load_dotenv

from feed_collector import FeedCollector
from surprise_analyzer import SurpriseAnalyzer
from x_collector import XCollector
from news_sources import X_SEARCH_KEYWORDS, X_ACCOUNTS

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    # ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
    load_dotenv()

    # å¿…é ˆç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
    required_vars = [
        'GROQ_API_KEY',
    ]

    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        logger.error(f"Missing required environment variables: {', '.join(missing_vars)}")
        sys.exit(1)

    # è¨­å®š
    timezone = os.getenv('TIMEZONE', 'Asia/Tokyo')
    hours_lookback = int(os.getenv('HOURS_LOOKBACK', '24'))

    logger.info("=== AI News Analyzer Started (Free Edition) ===")
    logger.info(f"Timezone: {timezone}")
    logger.info(f"Lookback period: {hours_lookback} hours")

    # ã‚¹ãƒ†ãƒƒãƒ—1: ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆRSS + Xï¼‰
    logger.info("\n[STEP 1] Collecting news from multiple sources...")

    # 1-1: RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰åé›†
    logger.info("[STEP 1-1] Collecting from RSS feeds...")
    collector = FeedCollector(timezone=timezone, hours_lookback=hours_lookback)
    rss_articles = collector.collect_all_feeds()
    logger.info(f"RSS articles collected: {len(rss_articles)}")

    # 1-2: Xã‹ã‚‰åé›†
    logger.info("[STEP 1-2] Collecting from X (Twitter)...")
    x_collector = XCollector(timezone=timezone, hours_lookback=hours_lookback)

    # Nitteræ¤œç´¢
    x_search_articles = x_collector.collect_from_search(X_SEARCH_KEYWORDS, max_tweets=50)
    logger.info(f"X search articles collected: {len(x_search_articles)}")

    # RSSHubï¼ˆç‰¹å®šã‚¢ã‚«ã‚¦ãƒ³ãƒˆï¼‰
    x_account_articles = x_collector.collect_from_rsshub(X_ACCOUNTS)
    logger.info(f"X account articles collected: {len(x_account_articles)}")

    # å…¨è¨˜äº‹ã‚’çµ±åˆ
    all_articles = rss_articles + x_search_articles + x_account_articles
    logger.info(f"Total articles collected: {len(all_articles)}")

    if not all_articles:
        logger.warning("No articles found in the specified time range")
        sys.exit(0)

    # AIé–¢é€£è¨˜äº‹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    ai_articles = [article for article in all_articles if collector.is_ai_related(article)]
    logger.info(f"AI-related articles: {len(ai_articles)} out of {len(all_articles)}")

    if not ai_articles:
        logger.warning("No AI-related articles found")
        sys.exit(0)

    # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚µãƒ—ãƒ©ã‚¤ã‚ºåº¦åˆ†æ
    logger.info("\n[STEP 2] Analyzing articles with Claude Code (Groq LLaMA 3.1 70B)...")
    analyzer = SurpriseAnalyzer(api_key=os.getenv('GROQ_API_KEY'))
    result = analyzer.analyze_articles(ai_articles)

    if not result:
        logger.error("Analysis failed")
        sys.exit(1)

    # çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info("\n=== Analysis Result ===")
    logger.info(f"Selected article: {result['article']['title']}")
    logger.info(f"Source: {result['article']['source']}")
    logger.info(f"URL: {result['article']['link']}")
    logger.info(f"Surprise score: {result['analysis'].get('surprise_score', 'N/A')}")

    # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_dir = "output"
    os.makedirs(output_dir, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_file = os.path.join(output_dir, f"analysis_{timestamp}.json")

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    logger.info(f"Result saved to: {output_file}")

    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆMarkdownå½¢å¼ï¼‰
    logger.info("\n[STEP 3] Generating detailed report...")
    report_file = os.path.join(output_dir, f"report_{timestamp}.md")
    generate_report(result, report_file)
    logger.info(f"Report saved to: {report_file}")

    logger.info("\n=== AI News Analyzer Completed ===")
    logger.info("Report will be posted to GitHub Issues by Actions workflow")


def generate_report(result: Dict, output_file: str):
    """
    è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’Markdownå½¢å¼ã§ç”Ÿæˆ

    Args:
        result: åˆ†æçµæœ
        output_file: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    article = result['article']
    analysis = result['analysis']

    report = f"""# AIãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

---

## ğŸš€ é¸å®šã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹

### ã‚¿ã‚¤ãƒˆãƒ«
{analysis.get('title_ja', article['title'])}

### ã‚½ãƒ¼ã‚¹
{article['source']} ({article['language']})

### URL
{article['link']}

### å…¬é–‹æ—¥æ™‚
{article['published'].strftime('%Y-%m-%d %H:%M %Z')}

---

## ğŸ“ æ¦‚è¦

{analysis.get('summary', article['summary'])}

---

## âš¡ ãªãœã‚µãƒ—ãƒ©ã‚¤ã‚ºã‹

"""

    # ã‚µãƒ—ãƒ©ã‚¤ã‚ºç†ç”±
    reasons = analysis.get('surprise_reasons', [])
    for i, reason in enumerate(reasons, 1):
        report += f"{i}. **{reason}**\n"

    report += f"""
---

## ğŸ’¡ ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆåˆ†æ

### ğŸ’» ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢è¦–ç‚¹

{analysis.get('engineer_impact', 'N/A')}

### ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹è¦–ç‚¹

{analysis.get('business_impact', 'N/A')}

---

## ğŸ“Š ã‚µãƒ—ãƒ©ã‚¤ã‚ºã‚¹ã‚³ã‚¢

**{analysis.get('surprise_score', 'N/A')} / 100**

---

## ğŸ” ä»–å€™è£œã¨ã®æ¯”è¼ƒ

{analysis.get('other_candidates_comparison', 'N/A')}

---

## ğŸ“‹ å…¨å€™è£œãƒªã‚¹ãƒˆ

"""

    # å…¨å€™è£œã‚’åˆ—æŒ™
    all_candidates = result.get('all_candidates', [])
    for i, candidate in enumerate(all_candidates, 1):
        report += f"""
### å€™è£œ{i}: {candidate['title'][:80]}
- **ã‚½ãƒ¼ã‚¹**: {candidate['source']}
- **URL**: {candidate['link']}
- **å…¬é–‹æ—¥æ™‚**: {candidate['published'].strftime('%Y-%m-%d %H:%M %Z')}
- **äºˆå‚™ã‚¹ã‚³ã‚¢**: {candidate.get('preliminary_score', 0)}

"""

    report += f"""
---

## ğŸ”§ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿

- **åˆ†æã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«**: Claude Code (Groq LLaMA 3.1 70B)
- **ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒ¢ãƒ¼ãƒ‰**: {'ã¯ã„' if result.get('fallback') else 'ã„ã„ãˆ'}
- **å€™è£œæ•°**: {len(all_candidates)}
- **åé›†ã‚½ãƒ¼ã‚¹**: RSS, X (Nitter), X (RSSHub)
"""

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)


if __name__ == "__main__":
    main()
